# LLM Tokenziers in C++ (0): Port from Python

## Why Tokenizer in C++

I am trying to run the HuggingFace GPT2 model on a smart phone. To tokenize the text data, I was supposed to use HuggingFace’s GPT2 tokenizer. Unfortunately, HuggingFace only offers the Python version and declined to offer a C/C++ version in [Mar 2020](https://github.com/huggingface/tokenizers/issues/185#issuecomment-594615029).

HuggingFace does offer a [Rust version](https://github.com/huggingface/tokenizers). But I'm worried that putting Rust into mobile development will create more problems.

According to [their document](https://docs.nvidia.com/deeplearning/nemo/user-guide/docs/en/stable/nlp/nemo_megatron/gpt/gpt_training.html#quick-start), NVIDIA NeMo project uses the Google Sentencepiece tokenizer library, which is in C++.  However, reading the source of HuggingFace [GPT2’s tokenizer](https://huggingface.co/docs/transformers/v4.26.0/en/model_doc/gpt2#transformers.GPT2Tokenizer), I noticed that it is of a different kind — a byte-pair encoding tokenizer. According to HuggingFace’s [nice guide](https://huggingface.co/docs/transformers/tokenizer_summary) to tokenizers, 

>Each model has its own tokenizer type. 
A pretrained model only performs properly if you feed it an input that was tokenized with the same rules that were used to tokenize its training data.


This guide also explains the three main types of tokenizers used with Transformer models: byte-pair encoding (BPE), WordPiece, and SentencePiece.  

Therefore, I decided to translate HuggingFace’s GPT2 tokenizer from Python to C++.

## What to Port

In order to figure out which part of the HuggingFace’s Python tokenizer project is what I need to port, I worte a simple Python script that calls the tokenizer, used `pdb` to step into the functions calls.  Then I noticed that [this eight-line Python method](https://github.com/huggingface/transformers/blob/5b49376202863d3798d2ff8a8ba61590542a1141/src/transformers/models/gpt2/tokenization_gpt2.py#L296-L304) is all what I need to translate if I am gonna run the GPT2 model.

```python
    def _tokenize(self, text):
        """Tokenize a string."""
        bpe_tokens = []
        for token in re.findall(self.pat, text):
            token = "".join(
                self.byte_encoder[b] for b in token.encode("utf-8")
            )  # Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)
            bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(" "))
        return bpe_tokens
```

From the above code snippet, we can see it does three things:

1. Roughly segment the input text using regular expression matching by calling `re.findall`.
1. Maps control bytes, b, in each candidate token into 255+b.
1. Maps each candidate token into one or more BPE tokens.

In some successive notes, I will explain how I port this three steps.  In the rest of this article, I will describe how I identified that the above function is all what I need to port.

## Install HuggingFace Transformers from Source Code

I git-cloned the HuggingFace's Transformers repository.

```bash
git clone https://github.com/huggingface/transformers
```

I followed the section
 [Editable Install](https://huggingface.co/docs/transformers/installation#editable-install), I ran the following command

```
cd transformers
pip install -e .
```

This process might complain about non-exisitng dependencies. Just install them.  After this process, we should be able to import `transformers`.

```bash
python -c 'import transformers'
```

## Drafting a Driving Script

I wrote this script `t.py` to tokenize a string as the following.

```python
import transformers
import builtins
from os import path

def load_gpt2_tokenizer() -> transformers.GPT2Tokenizer:
  builtins.open, tmp_open = open, builtins.open
  gpt2_dir = "/Users/y/w/gpt2cpp/assets"
  tokenizer = transformers.GPT2Tokenizer(
      vocab_file=path.join(gpt2_dir, 'vocab.json'),
      merges_file=path.join(gpt2_dir, 'merges.txt'))
  builtins.open = tmp_open
  return tokenizer

tknzr = load_gpt2_tokenizer()
print(tknzr("zero one two three four"))
```

My purpose is to use pdb to trace into the function call to `tknzr("zero one two three four")`.

## Class Hierarchy

Before showing the tracing result, it would be helpful to understand the class hierarchy.

1. [`class GPT2Tokenizer(PreTrainedTokenizer):`](https://github.com/huggingface/transformers/blob/5b49376202863d3798d2ff8a8ba61590542a1141/src/transformers/models/gpt2/tokenization_gpt2.py#LL104C12-L104C12)

1. [`class PreTrainedTokenizer(PreTrainedTokenizerBase):`](https://github.com/huggingface/transformers/blob/5b49376202863d3798d2ff8a8ba61590542a1141/src/transformers/tokenization_utils.py#LL333C7-L333C26)

1. [`class PreTrainedTokenizerBase(SpecialTokensMixin, PushToHubMixin):`](https://github.com/huggingface/transformers/blob/5b49376202863d3798d2ff8a8ba61590542a1141/src/transformers/tokenization_utils_base.py#L1476)

## Trace

Running the above driver script using the following command

```bash
python -m pdb t.py
```

reveals the following calling chain:

- [`PreTrainedTokenizerBase.__call__`](
https://github.com/huggingface/transformers/blob/5b49376202863d3798d2ff8a8ba61590542a1141/src/transformers/tokenization_utils_base.py#L2452-L2539)
  - [`PreTrainedTokenizerBase._switch_to_input_mode`](https://github.com/huggingface/transformers/blob/5b49376202863d3798d2ff8a8ba61590542a1141/src/transformers/tokenization_utils_base.py#L3564-L3568) is an empty implementation
  - [`PreTrainedTokenizerBase._call_one`](https://github.com/huggingface/transformers/blob/5b49376202863d3798d2ff8a8ba61590542a1141/src/transformers/tokenization_utils_base.py#L2541-L2651)
    - [`PreTrainedTokenizerBase.encode_plus`](https://github.com/huggingface/transformers/blob/5b49376202863d3798d2ff8a8ba61590542a1141/src/transformers/tokenization_utils_base.py#L2653-L2724)
      - [`PreTrainedTokenizerBase._get_padding_truncation_strategies`](https://github.com/huggingface/transformers/blob/5b49376202863d3798d2ff8a8ba61590542a1141/src/transformers/tokenization_utils_base.py#L2314-L2450)
        - Runs [`padding_strategy = PaddingStrategy.DO_NOT_PAD`](https://github.com/huggingface/transformers/blob/5b49376202863d3798d2ff8a8ba61590542a1141/src/transformers/tokenization_utils_base.py#L2372)
        - Runs [`truncation_strategy = TruncationStrategy.DO_NOT_TRUNCATE`](https://github.com/huggingface/transformers/blob/5b49376202863d3798d2ff8a8ba61590542a1141/src/transformers/tokenization_utils_base.py#LL2399C13-L2399C69)
        - Runs [`return padding_strategy, truncation_strategy, max_length=None, kwargs={}`](https://github.com/huggingface/transformers/blob/5b49376202863d3798d2ff8a8ba61590542a1141/src/transformers/tokenization_utils_base.py#L2450)
      - [`PreTrainedTokenizer._encode_plus`](https://github.com/huggingface/transformers/blob/5b49376202863d3798d2ff8a8ba61590542a1141/src/transformers/tokenization_utils.py#L593-L669)
        - [nested function `get_input_ids`](https://github.com/huggingface/transformers/blob/5b49376202863d3798d2ff8a8ba61590542a1141/src/transformers/tokenization_utils.py#L614C33-L638)
          - [`PreTrainedTokenizer.tokenize`](https://github.com/huggingface/transformers/blob/5b49376202863d3798d2ff8a8ba61590542a1141/src/transformers/tokenization_utils.py#L481-L549)
            - `all_special_tokens_extended = {'<|endoftext|>': AddedToken("<|endoftext|>", rstrip=False, lstrip=False, single_word=False, normalized=True)}`
            - [`text, kwargs = self.prepare_for_tokenization(text, **kwargs)
`](https://github.com/huggingface/transformers/blob/5b49376202863d3798d2ff8a8ba61590542a1141/src/transformers/tokenization_utils.py#L502) does nothing.
            - [`no_split_token = set(self.unique_no_split_tokens)`](https://github.com/huggingface/transformers/blob/5b49376202863d3798d2ff8a8ba61590542a1141/src/transformers/tokenization_utils.py#L516) returns `{}`
            - [`tokens = self.tokens_trie.split(text)`](https://github.com/huggingface/transformers/blob/5b49376202863d3798d2ff8a8ba61590542a1141/src/transformers/tokenization_utils.py#LL517C15-L517C15) returns `['zero one two three four']`
            - [`tokenized_text = []`](https://github.com/huggingface/transformers/blob/5b49376202863d3798d2ff8a8ba61590542a1141/src/transformers/tokenization_utils.py#L539)
            - [`tokenized_text.extend(self._tokenize(token))`](https://github.com/huggingface/transformers/blob/5b49376202863d3798d2ff8a8ba61590542a1141/src/transformers/tokenization_utils.py#L547)
              - [`GPT2Tokenizer._tokenize`](https://github.com/huggingface/transformers/blob/5b49376202863d3798d2ff8a8ba61590542a1141/src/transformers/models/gpt2/tokenization_gpt2.py#L296-L304)

Here we reached `GPT2Tokenizer._tokenize`. And before this invocation, all above stack frames do basically nothing.
